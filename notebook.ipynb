{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b9c3d7-a30b-4660-81bd-6944966f556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(token):\n",
    "    token = token.lower()\n",
    "    token = re.sub(\"[^a-z0-9]*\", \"\", token)\n",
    "    token = lemmatizer.lemmatize(token)\n",
    "\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6765b74d-f778-4ee7-96bb-f4f80027143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sent, max_length, token2id, UNK=1, PAD=0):\n",
    "    tokens = [token2id.get(clean_word(token), UNK) for token in sent.split()]\n",
    "\n",
    "    if len(tokens) < max_length:\n",
    "        diff = max_length - len(tokens)\n",
    "        tokens.extend([PAD] * diff)\n",
    "    elif len(tokens) > max_length:\n",
    "        tokens = tokens[:max_length]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f2c95a-3931-4a1a-b90b-5945268e575b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/matin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/matin/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Using custom data configuration default\n",
      "Reusing dataset quora (/home/matin/.cache/huggingface/datasets/quora/default/0.0.0/36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Define constant parameters\n",
    "GLOVE_PATH = '/home/matin/Workspace/Datasets-and-Models/GloVe/glove.6B.50d.txt'\n",
    "DATASET_NAME = 'quora'\n",
    "DATA_LEN = 30000\n",
    "MAX_LEN = 100\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "# Create variables\n",
    "token2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "counter = 2\n",
    "dataset = []\n",
    "s1, s2 = [], []\n",
    "\n",
    "# Load dataset\n",
    "data = datasets.load_dataset(DATASET_NAME, split='train')\n",
    "\n",
    "# Split data to a fixed size\n",
    "sentences = data['questions'][:DATA_LEN]\n",
    "labels = data['is_duplicate'][:DATA_LEN]\n",
    "\n",
    "# Iterate over sentences to split sentence 1 and 2\n",
    "for item in sentences:\n",
    "    s1.append(item['text'][0])\n",
    "    s2.append(item['text'][1])\n",
    "\n",
    "# Open GLoVe file for word embedding\n",
    "with open(GLOVE_PATH, 'r') as f:\n",
    "    raw_glove = f.read().strip().split('\\n')\n",
    "\n",
    "# Create an empty array to fill with glive embeddings\n",
    "glove_weights = np.zeros((len(raw_glove), len(raw_glove[0].split())-1), dtype=float)\n",
    "\n",
    "# Store values of embeddings and create dictionary with words and tokens\n",
    "for idx, item in enumerate(raw_glove):\n",
    "    for idy, entity in enumerate(item.split()):\n",
    "        if idy == 0:\n",
    "            token2idx[entity] = counter\n",
    "            counter += 1\n",
    "        else:\n",
    "            glove_weights[idx, idy-1] = float(entity)\n",
    "\n",
    "# Create a dataset with tokens and labels\n",
    "for idx, (sent1, sent2) in enumerate(zip(s1, s2)):\n",
    "    tag = 1 if labels[idx] == True else 0\n",
    "    dataset.append((tokenize_sentences(sent1, MAX_LEN, token2idx),\n",
    "                   tokenize_sentences(sent2, MAX_LEN, token2idx),\n",
    "                   tag))\n",
    "    \n",
    "# Shuffle dataset and split train and test\n",
    "shuffle(dataset)\n",
    "test_index = int(len(dataset)*TEST_SIZE)\n",
    "train = dataset[test_index:]\n",
    "test = dataset[:test_index]\n",
    "\n",
    "# Create x and y of train and test from dataset\n",
    "x_train1 = np.zeros((len(train), MAX_LEN), dtype=int)\n",
    "x_train2 = np.zeros((len(train), MAX_LEN), dtype=int)\n",
    "y_train = np.zeros((len(train)), dtype=int)\n",
    "x_test1 = np.zeros((len(test), MAX_LEN), dtype=int)\n",
    "x_test2 = np.zeros((len(test), MAX_LEN), dtype=int)\n",
    "y_test = np.zeros((len(test)), dtype=int)\n",
    "\n",
    "for idx, item in enumerate(train):\n",
    "    x_train1[idx, :] = item[0]\n",
    "    x_train2[idx, :] = item[1]\n",
    "    y_train[idx] = item[2]\n",
    "    \n",
    "    if idx <= len(test)-1:\n",
    "        x_test1[idx, :] = test[idx][0]\n",
    "        x_test2[idx, :] = test[idx][1]\n",
    "        y_test[idx] = test[idx][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0abd66b-815a-4bd4-9d00-c1dc8b012217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Embedding, Lambda, Concatenate\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.backend import abs as ab\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "(num_vocab, emb_dim) = glove_weights.shape\n",
    "lstm = Bidirectional(LSTM(20, dropout=0.2, recurrent_dropout=0.2))\n",
    "embed = Embedding(input_dim=num_vocab, output_dim=emb_dim, input_length=MAX_LEN, weights=[glove_weights], trainable=False)\n",
    "\n",
    "input1 = Input(shape=(MAX_LEN,))\n",
    "e1 = embed(input1)\n",
    "t1 = lstm(e1)\n",
    "\n",
    "input2 = Input(shape=(MAX_LEN,))\n",
    "e2 = embed(input2)\n",
    "t2 = lstm(e2)\n",
    "\n",
    "sub = lambda x: ab(x[0] - x[1])\n",
    "sub_layer = Lambda(function=sub, output_shape=20)([t1, t2])\n",
    "preds = Dense(1, activation='sigmoid')(sub_layer)\n",
    "model = Model(inputs=[input1, input2], outputs=preds)\n",
    "\n",
    "model.compile(loss=binary_crossentropy, optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b41ce03-2f39-4eec-bfbb-56c0222fee32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 100, 50)      20000000    ['input_10[0][0]',               \n",
      "                                                                  'input_11[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_5 (Bidirectional  (None, 40)          11360       ['embedding_5[0][0]',            \n",
      " )                                                                'embedding_5[1][0]']            \n",
      "                                                                                                  \n",
      " lambda_4 (Lambda)              (None, 40)           0           ['bidirectional_5[0][0]',        \n",
      "                                                                  'bidirectional_5[1][0]']        \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            41          ['lambda_4[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20,011,401\n",
      "Trainable params: 11,401\n",
      "Non-trainable params: 20,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1889522f-12f3-448a-8203-ef1d2ad4bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([x_train1, x_train2], y_train, epochs=20, validation_data=([x_test1, x_test2], y_test), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "622f805e-1739-48a1-82d8-d5c1a9041394",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
